{\rtf1\ansi\ansicpg949\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset129 AppleSDGothicNeo-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import warnings\
warnings.filterwarnings('ignore')\
\
import pandas as pd\
import numpy as np\
from sklearn.model_selection import train_test_split\
from sklearn.ensemble import RandomForestClassifier\
from sklearn.linear_model import LogisticRegression\
from sklearn.neighbors import KNeighborsClassifier\
from sklearn.tree import DecisionTreeClassifier\
from xgboost import XGBClassifier\
from lightgbm import LGBMClassifier\
\
from sklearn.model_selection import train_test_split,KFold,cross_val_score, RandomizedSearchCV\
from sklearn.metrics import confusion_matrix, f1_score\
from sklearn.metrics import classification_report\
\
import matplotlib.pyplot as plt\
from sklearn.metrics import accuracy_score\
import seaborn as sns\
\
#
\f1 train data\'b7\'ce \'b8\'f0\'b5\'a8\'b8\'b5 & \'b1\'b3\'c2\'f7\'b0\'cb\'c1\'f5
\f0 \
d = pd.read_csv('malware_train.csv')\
\
y_train = d['HasDetections']\
X_train = d[d.columns.difference(['HasDetections','MachineIdentifier'])] # HasDetection 
\f1 \'bf\'ad\'b0\'fa
\f0  id 
\f1 \'c1\'a6\'bf\'dc
\f0 \
\
#
\f1 \'b8\'f0\'b5\'a8
\f0  
\f1 \'bb\'fd\'bc\'ba
\f0 \
dt = DecisionTreeClassifier()\
rf = RandomForestClassifier()\
knn = KNeighborsClassifier(n_neighbors=35)\
lg = LogisticRegression()\
xgb = XGBClassifier()\
lgb = LGBMClassifier()\
\
#
\f1 \'b1\'b3\'c2\'f7\'b0\'cb\'c1\'f5\'b8\'f0\'b5\'a8\
from sklearn.model_selection import StratifiedShuffleSplit , cross_val_score\
ss = StratifiedShuffleSplit(n_splits=2,test_size=0.1,random_state=0)\
\
#kfold \'bc\'b3\'c1\'a4(k=10 -> 10\'b9\'f8\'c0\'c7 \'b1\'b3\'c2\'f7\'b0\'cb\'c1\'f5)\
from sklearn.model_selection import KFold\
kfold = KFold(n_splits=10, shuffle=True, random_state=0)\
\
#\'b8\'f0\'b5\'a8\'ba\'b0 \'b1\'b3\'c2\'f7\'b0\'cb\'c1\'f5 \'c1\'a4\'c8\'ae\'b5\'b5 \'c1\'a1\'bc\'f6 \'c0\'fa\'c0\'e5\
dt_cv = cross_val_score(dt,X_train,y_train, scoring=\'a1\'aeaccuracy\'a1\'af ,cv=kfold)\
rf_cv = cross_val_score(rf,X_train,y_train,scoring=\'a1\'aeaccuracy\'a1\'af ,cv=kfold)\
knn_cv = cross_val_score(knn,X_train,y_train,scoring=\'a1\'aeaccuracy\'a1\'af , cv=kfold)\
lg_cv = cross_val_score(lg,X_train,y_train,scoring=\'a1\'aeaccuracy\'a1\'af ,cv=kfold)\
xgb_cv = cross_val_score(xgb,X_train,y_train,scoring=\'a1\'aeaccuracy\'a1\'af ,cv=kfold)\
lgb_cv = cross_val_score(lgb,X_train,y_train, scoring=\'a1\'aeaccuracy\'a1\'af ,cv=kfold)\
\
#10\'b9\'f8 \'c1\'a1\'bc\'f6 print\
print('DT n_splits=\{\}, cross validation score: \{\}'.format(10, dt_cv))\
print('RF n_splits=\{\}, cross validation score: \{\}'.format(10, rf_cv))\
print('Knn n_splits=\{\}, cross validation score: \{\}'.format(10, knn_cv))\
print('Lg n_splits=\{\}, cross validation score: \{\}'.format(10, lg_cv))\
print('XGB n_splits=\{\}, cross validation score: \{\}'.format(10, xgb_cv))\
print('LGB n_splits=\{\}, cross validation score: \{\}'.format(10, lgb_cv))\
\
#10\'b9\'f8 \'c6\'f2\'b1\'d5 print\
print('DecisionTree.mean \\n\{:.3f\}'.format(dt_cv.mean()))\
print('RandomForest.mean \\n\{:.3f\}'.format(rf_cv.mean()))\
print('Knn.mean \\n\{:.3f\}'.format(knn_cv.mean()))\
print('LogisticRegression.mean \\n\{:.3f\}'.format(lg_cv.mean()))\
print('XGBoost.mean \\n\{:.3f\}'.format(xgb_cv.mean()))\
print('LightGBM.mean \\n\{:.3f\}'.format(lgb_cv.mean()))\
}